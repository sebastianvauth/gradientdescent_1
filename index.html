<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lesson 1: Gradient Descent - The Big Idea: Why and How?</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            font-size: 150%;
        }
        section {
            margin-bottom: 20px;
            padding: 20px;
            background-color: #ffffff;
            display: none;
            opacity: 0;
            transition: opacity 0.5s ease-in;
        }
        h1, h2, h3, h4 {
            color: #333;
            margin-top: 20px;
        }
        p, li {
            line-height: 1.6;
            color: #444;
            margin-bottom: 20px;
        }
        ul {
            padding-left: 20px;
        }
        .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think, .visual-aid-placeholder {
            text-align: left;
        }
        .image-placeholder img, .interactive-placeholder img, .visual-aid-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .vocab-section {
            background-color: #f0f8ff;
        }
        .vocab-section h3 {
            color: #1e90ff;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .vocab-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .vocab-term {
            font-weight: bold;
            color: #1e90ff;
        }
        .why-it-matters {
            background-color: #ffe6f0;
        }
        .why-it-matters h3 {
            color: #d81b60;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .stop-and-think {
            background-color: #e6e6ff;
        }
        .stop-and-think h3 {
            color: #4b0082;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .continue-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #007bff;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .reveal-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #4b0082;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .test-your-knowledge {
            background-color: #e6ffe6; /* Light green background */
        }
        .test-your-knowledge h3 {
            color: #28a745; /* Dark green heading */
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .test-your-knowledge h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .test-your-knowledge p {
            margin-bottom: 15px;
        }
        .check-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #28a745; /* Green background */
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
            border: none;
            font-size: 1em;
        }
        .faq-section {
            background-color: #fffbea; /* Light yellow background */
        }
        .faq-section h3 {
            color: #ffcc00; /* Bright yellow heading */
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .faq-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .math-explainer {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
        }
        .math-step {
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid #e9ecef;
        }
        .math-step:last-child {
            border-bottom: none;
        }
        .option {
            margin-bottom: 10px;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            cursor: pointer;
        }
        .option:hover {
            background-color: #f5f5f5;
        }
        .option-explanation {
            display: none;
            margin-top: 10px;
            padding: 10px;
            background-color: #f0f8ff;
            border-radius: 5px;
        }
        .correct {
            border-color: #28a745;
        }
        .incorrect {
            border-color: #dc3545;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <section id="section1">
     <div class="alexnet-arch interactive-element"> <h4>Explore the AlexNet Architecture</h4> <div class="alexnet-legend"> <div class="alexnet-legend-item"><span class="alexnet-dot alexnet-conv"></span> Convolution</div> <div class="alexnet-legend-item"><span class="alexnet-dot alexnet-pool"></span> Max Pooling</div> <div class="alexnet-legend-item"><span class="alexnet-dot alexnet-fc"></span> Fully Connected</div> <div class="alexnet-legend-item"><span class="alexnet-dot alexnet-cross"></span> Cross-GPU Connection</div> </div> <div class="alexnet-grid"> <div class="alexnet-column"> <div class="alexnet-column-title">GPU 1</div> <div class="alexnet-stream" id="alexnet-gpu1"></div> </div> <div class="alexnet-column"> <div class="alexnet-column-title">GPU 2</div> <div class="alexnet-stream" id="alexnet-gpu2"></div> </div> </div> <div class="alexnet-note"> Tip: Click a layer to view its parameters. Notice how spatial dimensions shrink and channel depth grows as you move deeper. </div> </div> <style> /* Scoped styles for the interactive; relies on existing CSS variables from the page */ .alexnet-arch { background: var(--light-gray); border: 2px dashed #6c757d; border-radius: 8px; padding: 20px; transition: var(--transition); } .alexnet-arch:hover { border-color: var(--primary-blue); background-color: #f0f8ff; } .alexnet-arch h4 { color: #495057; margin-bottom: 12px; } .alexnet-legend { display: flex; flex-wrap: wrap; gap: 12px; margin-bottom: 16px; color: var(--text-secondary); font-size: 0.95em; } .alexnet-legend-item { display: inline-flex; align-items: center; gap: 8px; background: #fff; border: 1px solid var(--border-gray); border-radius: 999px; padding: 6px 10px; box-shadow: var(--shadow-sm); } .alexnet-dot { width: 10px; height: 10px; border-radius: 999px; display: inline-block; } .alexnet-conv { background: var(--primary-blue); } .alexnet-pool { background: #6c757d; } .alexnet-fc { background: #28a745; } .alexnet-cross{ background: #ff8a00; } .alexnet-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 16px; } .alexnet-column { background: #fff; border: 1px solid var(--border-gray); border-radius: 8px; padding: 12px; box-shadow: var(--shadow-sm); } .alexnet-column-title { color: var(--text-primary); font-weight: 600; margin-bottom: 10px; } .alexnet-stream { display: grid; gap: 12px; position: relative; } .alexnet-card { background: var(--light-gray); border: 2px solid var(--border-gray); border-radius: 6px; padding: 10px 12px; box-shadow: var(--shadow-sm); cursor: pointer; transition: var(--transition); position: relative; } .alexnet-card:hover { background: var(--hover-gray); border-color: #adb5bd; transform: translateY(-1px); box-shadow: var(--shadow-md); } .alexnet-card:focus { outline: 2px solid var(--primary-blue); outline-offset: 2px; } .alexnet-card-title { font-weight: 700; color: var(--text-primary); margin-bottom: 4px; display: flex; align-items: center; gap: 8px; } .alexnet-chip { display: inline-flex; align-items: center; gap: 6px; font-size: 0.8em; color: #fff; padding: 2px 8px; border-radius: 999px; } .chip-conv { background: var(--primary-blue); } .chip-pool { background: #6c757d; } .chip-fc { background: #28a745; } .alexnet-sub { font-size: 0.9em; color: var(--text-secondary); } .alexnet-popover { display: none; margin-top: 8px; background: #fff; border: 1px solid var(--border-gray); border-radius: 6px; box-shadow: var(--shadow-md); padding: 10px; animation: fadeIn 0.2s ease-in; } .alexnet-popover.open { display: block; } .alexnet-popover-title { font-weight: 600; color: var(--text-primary); margin-bottom: 6px; } .alexnet-kv { display: grid; grid-template-columns: 140px 1fr; gap: 6px 12px; font-size: 0.95em; } .alexnet-kv strong { color: var(--text-primary); } .alexnet-note { margin-top: 16px; font-size: 0.95em; color: var(--text-secondary); } /* Connectors */ .alexnet-connector { height: 20px; width: 2px; background: var(--border-gray); margin: 0 auto; border-radius: 2px; } /* Cross-GPU connector indicator */ .alexnet-cross-connector { display: flex; align-items: center; gap: 8px; color: #ff8a00; font-size: 0.9em; padding: 6px 8px; background: #fff7e6; border: 1px solid #ffd8a8; border-radius: 6px; box-shadow: var(--shadow-sm); margin: 6px 0; } @media (max-width: 768px) { .alexnet-grid { grid-template-columns: 1fr; } } </style> <script> (function() { // Data model for AlexNet layers (simplified, illustrative) const layers = [ // GPU 1 stream { id: 'conv1_g1', name: 'Conv1', gpu: 1, type: 'Convolution', kernels: 96, kernelSize: '11×11', stride: 4, padding: 0, activation: 'ReLU', notes: 'LRN after Conv1', connectsTo: ['pool1_g1'] }, { id: 'pool1_g1', name: 'Pool1', gpu: 1, type: 'Max Pooling', kernels: null, kernelSize: '3×3', stride: 2, padding: 0, activation: '—', notes: '', connectsTo: ['conv2_g1'] }, { id: 'conv2_g1', name: 'Conv2', gpu: 1, type: 'Convolution', kernels: 256, kernelSize: '5×5', stride: 1, padding: 2, activation: 'ReLU', notes: 'LRN after Conv2', connectsTo: ['pool2_g1'] }, { id: 'pool2_g1', name: 'Pool2', gpu: 1, type: 'Max Pooling', kernels: null, kernelSize: '3×3', stride: 2, padding: 0, activation: '—', notes: '', connectsTo: ['conv3_g1'] }, { id: 'conv3_g1', name: 'Conv3', gpu: 1, type: 'Convolution', kernels: 384, kernelSize: '3×3', stride: 1, padding: 1, activation: 'ReLU', notes: 'Receives from both GPUs', connectsTo: ['conv4_g1'], crossConnect: true }, { id: 'conv4_g1', name: 'Conv4', gpu: 1, type: 'Convolution', kernels: 384, kernelSize: '3×3', stride: 1, padding: 1, activation: 'ReLU', notes: '', connectsTo: ['conv5_g1'] }, { id: 'conv5_g1', name: 'Conv5', gpu: 1, type: 'Convolution', kernels: 256, kernelSize: '3×3', stride: 1, padding: 1, activation: 'ReLU', notes: '', connectsTo: ['pool3_g1'] }, { id: 'pool3_g1', name: 'Pool3', gpu: 1, type: 'Max Pooling', kernels: null, kernelSize: '3×3', stride: 2, padding: 0, activation: '—', notes: 'Leads to FC stack', connectsTo: [] }, // GPU 2 stream (mirrored) { id: 'conv1_g2', name: 'Conv1', gpu: 2, type: 'Convolution', kernels: 96, kernelSize: '11×11', stride: 4, padding: 0, activation: 'ReLU', notes: 'LRN after Conv1', connectsTo: ['pool1_g2'] }, { id: 'pool1_g2', name: 'Pool1', gpu: 2, type: 'Max Pooling', kernels: null, kernelSize: '3×3', stride: 2, padding: 0, activation: '—', notes: '', connectsTo: ['conv2_g2'] }, { id: 'conv2_g2', name: 'Conv2', gpu: 2, type: 'Convolution', kernels: 256, kernelSize: '5×5', stride: 1, padding: 2, activation: 'ReLU', notes: 'LRN after Conv2', connectsTo: ['pool2_g2'] }, { id: 'pool2_g2', name: 'Pool2', gpu: 2, type: 'Max Pooling', kernels: null, kernelSize: '3×3', stride: 2, padding: 0, activation: '—', notes: '', connectsTo: ['conv3_g2'] }, { id: 'conv3_g2', name: 'Conv3', gpu: 2, type: 'Convolution', kernels: 384, kernelSize: '3×3', stride: 1, padding: 1, activation: 'ReLU', notes: 'Receives from both GPUs', connectsTo: ['conv4_g2'], crossConnect: true }, { id: 'conv4_g2', name: 'Conv4', gpu: 2, type: 'Convolution', kernels: 384, kernelSize: '3×3', stride: 1, padding: 1, activation: 'ReLU', notes: '', connectsTo: ['conv5_g2'] }, { id: 'conv5_g2', name: 'Conv5', gpu: 2, type: 'Convolution', kernels: 256, kernelSize: '3×3', stride: 1, padding: 1, activation: 'ReLU', notes: '', connectsTo: ['pool3_g2'] }, { id: 'pool3_g2', name: 'Pool3', gpu: 2, type: 'Max Pooling', kernels: null, kernelSize: '3×3', stride: 2, padding: 0, activation: '—', notes: 'Leads to FC stack', connectsTo: [] }, // FC layers (logical, shared head) { id: 'fc1', name: 'FC1', gpu: null, type: 'Fully Connected', kernels: 4096, kernelSize: '—', stride: '—', padding: '—', activation: 'ReLU', notes: '', connectsTo: ['fc2'] }, { id: 'fc2', name: 'FC2', gpu: null, type: 'Fully Connected', kernels: 4096, kernelSize: '—', stride: '—', padding: '—', activation: 'ReLU', notes: '', connectsTo: ['fc3'] }, { id: 'fc3', name: 'FC3', gpu: null, type: 'Fully Connected (Output)', kernels: 1000, kernelSize: '—', stride: '—', padding: '—', activation: 'Softmax', notes: '', connectsTo: [] }, ]; const gpu1Order = ['conv1_g1', 'pool1_g1', 'conv2_g1', 'pool2_g1', 'conv3_g1', 'conv4_g1', 'conv5_g1', 'pool3_g1']; const gpu2Order = ['conv1_g2', 'pool1_g2', 'conv2_g2', 'pool2_g2', 'conv3_g2', 'conv4_g2', 'conv5_g2', 'pool3_g2']; const fcOrder = ['fc1', 'fc2', 'fc3']; const gpu1Container = document.getElementById('alexnet-gpu1'); const gpu2Container = document.getElementById('alexnet-gpu2'); function makeCard(layer) { const card = document.createElement('div'); card.className = 'alexnet-card'; card.setAttribute('tabindex', '0'); card.setAttribute('role', 'button'); card.setAttribute('aria-expanded', 'false'); card.dataset.layerId = layer.id; const chipClass = layer.type === 'Convolution' ? 'chip-conv' : layer.type === 'Max Pooling' ? 'chip-pool' : 'chip-fc'; const title = document.createElement('div'); title.className = 'alexnet-card-title'; title.innerHTML = ` <span>${layer.name}</span> <span class="alexnet-chip ${chipClass}">${layer.type}</span> `; const sub = document.createElement('div'); sub.className = 'alexnet-sub'; sub.textContent = layer.activation && layer.activation !== '—' ? `Activation: ${layer.activation}` : 'Details'; const pop = document.createElement('div'); pop.className = 'alexnet-popover'; pop.id = `popover-${layer.id}`; pop.setAttribute('role', 'region'); pop.setAttribute('aria-label', `${layer.name} details`); pop.innerHTML = ` <div class="alexnet-popover-title">${layer.name} Parameters</div> <div class="alexnet-kv"> <strong>Type</strong><div>${layer.type}</div> <strong>Kernels/Channels</strong><div>${layer.kernels ?? '—'}</div> <strong>Kernel Size</strong><div>${layer.kernelSize}</div> <strong>Stride</strong><div>${layer.stride}</div> <strong>Padding</strong><div>${layer.padding}</div> <strong>Activation</strong><div>${layer.activation}</div> ${layer.notes ? `<strong>Notes</strong><div>${layer.notes}</div>` : ''} </div> `; card.appendChild(title); card.appendChild(sub); card.appendChild(pop); function togglePopover(forceState) { const isOpen = pop.classList.contains('open'); closeAllPopovers(); if (forceState === true || (!isOpen && forceState !== false)) { pop.classList.add('open'); card.setAttribute('aria-expanded', 'true'); } else { pop.classList.remove('open'); card.setAttribute('aria-expanded', 'false'); } } card.addEventListener('click', (e) => { e.stopPropagation(); togglePopover(); }); card.addEventListener('keydown', (e) => { if (e.key === 'Enter' || e.key === ' ') { e.preventDefault(); togglePopover(); } else if (e.key === 'Escape') { e.preventDefault(); pop.classList.remove('open'); card.setAttribute('aria-expanded', 'false'); } }); return card; } function addConnector(container) { const conn = document.createElement('div'); conn.className = 'alexnet-connector'; container.appendChild(conn); } function addCrossConnector(container, text) { const banner = document.createElement('div'); banner.className = 'alexnet-cross-connector'; banner.innerHTML = ` <span class="alexnet-dot alexnet-cross"></span> <span>${text}</span> `; container.appendChild(banner); } function closeAllPopovers() { document.querySelectorAll('.alexnet-popover.open').forEach(p => { p.classList.remove('open'); const parent = p.closest('.alexnet-card'); if (parent) parent.setAttribute('aria-expanded', 'false'); }); } // Render GPU 1 stream gpu1Order.forEach((id, idx) => { const layer = layers.find(l => l.id === id); if (!layer) return; const card = makeCard(layer); gpu1Container.appendChild(card); // Add cross-connector notice at Conv3 if (layer.crossConnect) { addCrossConnector(gpu1Container, 'Cross-GPU aggregation occurs here'); } // Connector between nodes (except last) if (idx < gpu1Order.length - 1) addConnector(gpu1Container); }); // Render GPU 2 stream gpu2Order.forEach((id, idx) => { const layer = layers.find(l => l.id === id); if (!layer) return; const card = makeCard(layer); gpu2Container.appendChild(card); if (layer.crossConnect) { addCrossConnector(gpu2Container, 'Cross-GPU aggregation occurs here'); } if (idx < gpu2Order.length - 1) addConnector(gpu2Container); }); // Append FC stack below both columns visually by adding a shared section beneath // Create a shared row under the two columns to house FC layers if desired in-page: const fcMount = document.createElement('div'); fcMount.style.gridColumn = '1 / -1'; fcMount.style.marginTop = '16px'; const fcWrapper = document.createElement('div'); fcWrapper.className = 'alexnet-column'; const title = document.createElement('div'); title.className = 'alexnet-column-title'; title.textContent = 'Fully Connected Head (Shared)'; fcWrapper.appendChild(title); const fcStream = document.createElement('div'); fcStream.className = 'alexnet-stream'; fcWrapper.appendChild(fcStream); fcOrder.forEach((id, idx) => { const layer = layers.find(l => l.id === id); const card = makeCard(layer); fcStream.appendChild(card); if (idx < fcOrder.length - 1) addConnector(fcStream); }); // Insert FC wrapper after the grid const grid = document.querySelector('.alexnet-grid'); grid.parentNode.insertBefore(fcMount, grid.nextSibling); fcMount.appendChild(fcWrapper); // Close popovers on outside click document.addEventListener('click', (e) => { if (!e.target.closest('.alexnet-card')) { closeAllPopovers(); } }); })(); </script>
        <h1>Lesson 1: Gradient Descent - The Big Idea: Why and How?</h1>
        <p>Imagine you're trying to train a machine learning model on a massive dataset – maybe millions of photos or customer records. Calculating the <em>absolute best</em> answer in one go often requires loading and processing <em>all</em> that data simultaneously. That can be incredibly slow, demand huge amounts of memory, or sometimes, it's just plain impossible!</p>
        <p>So, how do we find good settings (parameters) for our model efficiently, even with enormous datasets? We need a clever approach. Enter <strong>Gradient Descent</strong>!</p>
        <div class="continue-button" onclick="showNextSection(2)">Continue</div>
    </section>

    <section id="section2">
        <h2>Why Not Just Calculate the Best Answer Directly?</h2>
        <p>For some problems, like simpler linear regression, there's a mathematical formula (the 'least squares solution' or 'normal equation') that gives you the perfect answer directly. But this requires performing complex calculations involving your <em>entire</em> dataset, let's call it \(D\).</p>
        <div class="visual-aid-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Visual Aid: Simple diagram contrasting two paths. Path 1: 'Direct Calculation (Least Squares)' -> requires 'Entire Dataset D' -> 'Exact Solution β_optimal'. Path 2: 'Gradient Descent' -> uses 'Data (iteratively)' -> 'Approximated Solution β_learned'. Highlight that Path 1 struggles with 'Large D'.">
        </div>
        <p>As your dataset \(D\) grows, these direct calculations become computationally very expensive. Think matrix operations on millions of data points! It quickly becomes impractical.</p>
        <div class="continue-button" onclick="showNextSection(3)">Continue</div>
    </section>

    <section id="section3">
        <p>Gradient Descent offers an alternative: it's an <strong>approximated learning method</strong>. It might not find the <em>absolute perfect</em> answer down to the last decimal place, but it can often get extremely close, much, much faster, especially with large datasets.</p>
        <div class="continue-button" onclick="showNextSection(4)">Continue</div>
    </section>

    <section id="section4">
        <h2>The Iterative Journey Downhill</h2>
        <p>So, how does Gradient Descent work? Instead of a giant calculation, it takes an iterative approach. Think of finding the lowest point in a valley shrouded in thick fog.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Animated GIF: A ball rolling down a simple curved slope (valley shape), taking discrete steps.">
        </div>
        <p>You can't see the bottom (the best parameters), but you can feel the slope right where you are standing. To get lower, you'd take a step in the steepest <em>downhill</em> direction.</p>
        <div class="continue-button" onclick="showNextSection(5)">Continue</div>
    </section>

    <section id="section5">
        <p>Gradient Descent does exactly this, digitally:</p>
        <ol>
            <li><strong>Start Somewhere:</strong> Make an initial guess for your model's parameters (we often call this vector \(\beta_{start}\) or \(\theta_{start}\)). This could be random numbers, or just all zeros.</li>
            <li><strong>Take a Step Downhill:</strong> Figure out which direction increases the 'error' or 'cost' the most (this direction is called the <strong>gradient</strong>). Then, take a small step in the <em>opposite</em> direction to reduce the error.</li>
            <li><strong>Repeat:</strong> Keep taking small steps downhill, iteratively adjusting the parameters, until you're satisfied you're near the bottom.</li>
        </ol>
        <div class="continue-button" onclick="showNextSection(6)">Continue</div>
    </section>

    <section id="section6">
        <h2>Visualizing the Goal</h2>
        <p>Let's visualize this for a simple case. Imagine our 'cost' (how bad our parameters are) depends on just one parameter, \(\beta\). We can plot this relationship.</p>
        <div class="visual-aid-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Visual Aid: Show the 1D convex cost curve from Slide 1. Label the axes 'Parameter (β)' and 'Cost c(β)'. Mark a point high up on the right side as 'β_start'. Mark the bottom of the curve as 'β_learned (Goal)'. Add arrows pointing downhill along the curve from β_start towards β_learned, representing the iterative steps.">
        </div>
        <p>Here, the curve \(c(\beta)\) shows the cost for different values of our parameter \(\beta\). Our goal is to find the value of \(\beta\) at the bottom of this 'valley', which minimizes the cost. We start at some \(\beta_{start}\) and take steps downhill until we reach (or get very close to) \(\beta_{learned}\).</p>
        <div class="continue-button" onclick="showNextSection(7)">Continue</div>
    </section>

    <section id="section7">
        <h2>The Update Rule: Math Behind the Steps</h2>
        <p>Okay, let's formalize that 'step downhill'. How do we calculate the next set of parameters (\(\beta_{new}\)) from the current ones (\(\beta_{old}\))? The core Gradient Descent update rule is surprisingly simple:</p>
                <p>\[ \beta_{new} = \beta_{old} - \alpha \nabla_{\beta} C(\beta_{old}) \]</p>
                <p>This is the heart of Gradient Descent. Let's break it down piece by piece.</p>
<div class="continue-button" onclick="showNextSection(8)">Continue</div>
    </section>

    <section id="section8">
       
                    
            
          
                <p>\[ \beta \]</p>
                <p>This represents the vector of parameters (or coefficients) we are trying to learn for our model. \(\beta_{old}\) is our current guess, and \(\beta_{new}\) will be our improved guess after one step.</p>
          
            <div class="continue-button" onclick="showNextSection(9)">Continue</div>
    </section>

    <section id="section9">
         
                <p>\[ C(\beta) \]</p>
                <p>This is the <strong>Cost Function</strong>. It's a mathematical function that measures how 'wrong' our model's predictions are given the current parameters \(\beta\). Our goal is to find the \(\beta\) that makes \(C(\beta)\) as small as possible (minimize the cost).</p>
          
            
           <div class="continue-button" onclick="showNextSection(10)">Continue</div>
    </section>

    <section id="section10">
                <p>\[ \nabla_{\beta} C(\beta) \]</p>
                <p>This is the <strong>Gradient</strong> of the cost function with respect to the parameters \(\beta\). Think of it as a multi-dimensional slope. It's a vector that points in the direction where the cost function \(C(\beta)\) increases <em>fastest</em> from the current point \(\beta\). We calculate this gradient at our current position, \(\beta_{old}\).</p>
        
            <div class="continue-button" onclick="showNextSection(11)">Continue</div>
    </section>

    <section id="section11">
            
                <p>\[ \alpha \]</p>
                <p>This is the <strong>Learning Rate</strong>. It's a small positive number (e.g., 0.01, 0.005, 0.1) that we choose beforehand. It controls <em>how big</em> of a step we take in the downhill direction. It's crucial: too large, and we might overshoot the minimum; too small, and it might take ages to get there!</p>
   
            <div class="continue-button" onclick="showNextSection(12)">Continue</div>
    </section>

    <section id="section12">
     
                <p>\[ - \alpha \nabla_{\beta} C(\beta_{old}) \]</p>
                <p>This is the actual 'step' we take. We move in the direction <em>opposite</em> to the gradient (hence the minus sign – we want to go downhill, not uphill!), and the size of the step is determined by the gradient's magnitude and scaled by our chosen learning rate \(\alpha\).</p>
       
 
        <div class="continue-button" onclick="showNextSection(13)">Continue</div>
    </section>

    <section id="section13">
        <p>So, in each iteration, we calculate the gradient based on our current parameters, scale it by the learning rate, subtract it from our current parameters, and voilà – we have our new, hopefully slightly better, parameters!</p>
        <div class="continue-button" onclick="showNextSection(14)">Continue</div>
    </section>

    <section id="section14">
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Parameters (β or θ)</h4>
            <p>The adjustable settings or coefficients of a machine learning model that are learned from data (e.g., the slope and intercept in linear regression).</p>
            
            <h4 class="vocab-term">Cost Function (C(β))</h4>
            <p>A function that measures the difference between the model's predictions and the actual target values. The goal is to minimize this function.</p>
            
            <h4 class="vocab-term">Gradient (∇C)</h4>
            <p>A vector of partial derivatives of the cost function with respect to each parameter. It points in the direction of the steepest ascent of the cost function.</p>
            
            <h4 class="vocab-term">Learning Rate (α)</h4>
            <p>A hyperparameter (a setting chosen before learning starts) that controls the step size taken during each iteration of gradient descent.</p>
            
            <h4 class="vocab-term">Gradient Descent</h4>
            <p>An iterative optimization algorithm used to find the minimum of a function (typically a cost function) by repeatedly taking steps in the opposite direction of the gradient.</p>
        </div>
        
        <div class="continue-button" onclick="showNextSection(15)">Continue</div>
    </section>

    <section id="section15">
        <h2>Exploring the Descent</h2>
        <p>Let's play with this idea. Use the interactive element below to see how the starting point and learning rate affect the descent on our simple cost curve.</p>
        
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Interactive Element: Convex Cost Function Explorer. Description: Display the 1D convex cost curve $c(\\beta)$ vs $\\beta$. Provide a slider or clickable input for the user to set the 'Starting Beta' (β_start). Provide another slider for the 'Learning Rate' (α) with values ranging from very small (e.g., 0.01) to too large (e.g., 1.5 based on the curve's scale). Add a 'Start Descent' button. When clicked, animate points moving down the curve according to the update rule β := β - α * (slope at β). Show the value of β and C(β) at each step. If α is too large, show the animation overshooting the minimum or diverging (going up). If α is small, show slow convergence. If α is 'just right', show convergence to the minimum.">
        </div>
        <div class="continue-button" onclick="showNextSection(17)">Continue</div>
    </section>

    <section id="section17">
        <p>Did you see how a tiny learning rate takes many small steps? And how a huge learning rate can cause the process to jump around wildly or even move <em>away</em> from the minimum? Finding a good learning rate is often key!</p>
        <div class="continue-button" onclick="showNextSection(18)">Continue</div>
    </section>

    <section id="section18">
        <div class="test-your-knowledge">
            <h3>Test Your Knowledge</h3>
            <h4>Based on the update rule \(\beta_{new} = \beta_{old} - \alpha \nabla_{\beta} C(\beta_{old})\), what happens if the learning rate \(\alpha\) is set to 0?</h4>
            
            <div class="option" onclick="checkAnswer(this, false)">
                The parameters \(\beta\) update very quickly.
                <div class="option-explanation">If α=0, the entire update step (- α * gradient) becomes zero.</div>
            </div>
            
            <div class="option" onclick="checkAnswer(this, true)">
                The parameters \(\beta\) do not change.
                <div class="option-explanation">Correct! If α=0, then β_new = β_old - 0 * gradient = β_old. No learning occurs.</div>
            </div>
            
            <div class="option" onclick="checkAnswer(this, false)">
                The algorithm finds the minimum in one step.
                <div class="option-explanation">Finding the minimum requires moving; a step size of zero means no movement.</div>
            </div>
            
            <div class="option" onclick="checkAnswer(this, false)">
                The algorithm moves in the direction of the gradient.
                <div class="option-explanation">The algorithm moves opposite the gradient, but if α=0, the step size is zero regardless of the gradient.</div>
            </div>
        </div>
                <div class="continue-button" onclick="showNextSection(19)">Continue</div>
    </section>

    <section id="section19">
<div class="why-it-matters">
            <h3>Why It Matters</h3>
            <p>Gradient Descent provides a general and scalable way to optimize the parameters of many machine learning models, especially when datasets are too large for direct calculation methods. It's the foundation for training many algorithms, from simple linear regression to complex deep neural networks.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(20)">Continue</div>
    </section>

    <section id="section20">
<div class="faq-section">
            <h3>Frequently Asked Questions</h3>
            <h4>Does it matter where I start (β_start)?</h4>
            <p>For simple 'bowl-shaped' (convex) cost functions like the one we visualized, you'll eventually reach the same minimum regardless of where you start (though it might take longer from further away). However, for more complex, bumpy cost functions (which we'll see in Lesson 3), the starting point can determine <em>which</em> minimum you fall into!</p>
        </div>

        <div class="continue-button" onclick="showNextSection(21)">Continue</div>
    </section>

    <section id="section21">
        <h2>Next Steps</h2>
        
        
        
     
        <p>We've grasped the core concept: start somewhere, find the downhill direction (gradient), take a step opposite to it (scaled by the learning rate), and repeat.</p>
        
        <p>But how do we <em>actually calculate</em> that gradient \(\nabla_{\beta} C(\beta)\) for a specific model like linear regression? That's exactly what we'll dive into in the next lesson!</p>
        
    
    </section>

    <script>
        // Show the first section initially
        document.getElementById("section1").style.display = "block";
        document.getElementById("section1").style.opacity = "1";

        function showNextSection(nextSectionId) {
            const currentButton = event.target;
            const nextSection = document.getElementById("section" + nextSectionId);
            
            currentButton.style.display = "none";
            
            nextSection.style.display = "block";
            setTimeout(() => {
                nextSection.style.opacity = "1";
            }, 10);

            setTimeout(() => {
                nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 500);
        }

        function checkAnswer(element, isCorrect) {
            // Remove any previous correct/incorrect classes
            const options = document.querySelectorAll('.option');
            options.forEach(option => {
                option.classList.remove('correct', 'incorrect');
            });
            
            // Add the appropriate class
            if (isCorrect) {
                element.classList.add('correct');
            } else {
                element.classList.add('incorrect');
            }
            
            // Show the explanation
            const explanation = element.querySelector('.option-explanation');
            explanation.style.display = 'block';
            
            // Hide other explanations
            options.forEach(option => {
                if (option !== element) {
                    const otherExplanation = option.querySelector('.option-explanation');
                    otherExplanation.style.display = 'none';
                }
            });
        }
    </script>
</body>
</html>
